{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0u6xQMKCOYz",
        "outputId": "f80f9301-088b-4c2b-c806-427debc0cfef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May  2 12:44:01 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.14              Driver Version: 551.78         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4070 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   43C    P8              9W /  285W |    1220MiB /  12282MiB |      8%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A        25      G   /Xwayland                                   N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "q9efInp3Wlup"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "744.30s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pypdf python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWPitUwP817",
        "outputId": "c2fba8a9-e38f-49e2-d4be-8e44c5fd5298"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "753.87s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HRlj4A4QNjz",
        "outputId": "4133b808-839f-4ced-ac1c-b69d3e135007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "775.10s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets loralib sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-wFP3ngwWwLo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "787.49s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install -q einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiRYz1k0c3B-",
        "outputId": "8769eb00-4372-472f-9042-2e8f621b5c91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "954.62s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (4.41.0.dev0)\n",
            "Requirement already satisfied: tqdm in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: numpy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (1.26.4)\n",
            "Collecting scikit-learn (from sentence_transformers)\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy (from sentence_transformers)\n",
            "  Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (0.22.2)\n",
            "Requirement already satisfied: Pillow in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence_transformers) (10.3.0)\n",
            "Requirement already satisfied: filelock in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.4.28)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn, sentence_transformers\n",
            "Successfully installed scikit-learn-1.4.2 scipy-1.13.0 sentence_transformers-2.7.0 threadpoolctl-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxTtFeeUW5WJ",
        "outputId": "b09de2aa-61ee-4f35-bc52-befc018d2c84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "664.03s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.33-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.3-py3-none-any.whl.metadata (678 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.32 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.33-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl.metadata (603 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.16-py3-none-any.whl.metadata (559 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl.metadata (677 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.19-py3-none-any.whl.metadata (979 bytes)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
            "  Downloading openai-1.25.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.32->llama-index) (1.6.0)\n",
            "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numpy (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting requests>=2.31.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.32->llama-index) (4.11.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi (from httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting idna (from httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading regex-2024.4.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.32->llama-index) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.32->llama-index) (24.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.18.2 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.32->llama-index)\n",
            "  Downloading pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.32->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.10.33-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_index_agent_openai-0.2.3-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_core-0.10.33-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.16-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.19-py3-none-any.whl (36 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
            "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.25.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.0/620.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.18.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.4.28-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.2/785.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, pytz, dirtyjson, wrapt, urllib3, tzdata, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, pypdf, pydantic-core, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 attrs-23.2.0 beautifulsoup4-4.12.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.5 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.3.1 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 joblib-1.4.0 llama-index-0.10.33 llama-index-agent-openai-0.2.3 llama-index-cli-0.1.12 llama-index-core-0.10.33 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.16 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.19 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.2 llamaindex-py-client-0.1.19 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.3 nltk-3.8.1 numpy-1.26.4 openai-1.25.0 pandas-2.2.2 pillow-10.3.0 pydantic-2.7.1 pydantic-core-2.18.2 pypdf-4.2.0 pytz-2024.1 regex-2024.4.28 requests-2.31.0 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.2 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "s8AN-3q1eZ7p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "704.91s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvuPlitEljTY",
        "outputId": "b4f429e7-caa5-4594-f643-26622378bc5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "724.32s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-llms-ollama\n",
            "  Downloading llama_index_llms_ollama-0.1.3-py3-none-any.whl.metadata (585 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-llms-ollama) (0.10.33)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.3.1)\n",
            "Requirement already satisfied: httpx in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.8.1)\n",
            "Requirement already satisfied: numpy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.25.0)\n",
            "Requirement already satisfied: pandas in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (10.3.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.9.4)\n",
            "Requirement already satisfied: pydantic>=1.10 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.7.1)\n",
            "Requirement already satisfied: anyio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.3.0)\n",
            "Requirement already satisfied: certifi in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.14.0)\n",
            "Requirement already satisfied: click in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.4.28)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.16.0)\n",
            "Downloading llama_index_llms_ollama-0.1.3-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: llama-index-llms-ollama\n",
            "Successfully installed llama-index-llms-ollama-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-llms-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcXk_Dm_mXt_",
        "outputId": "a2878bbf-3d5f-41d0-8a4c-8e4599b3a7a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "966.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.0-py3-none-any.whl.metadata (725 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.22.2)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (0.10.33)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0,>=2.6.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (2.7.0)\n",
            "Requirement already satisfied: filelock in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\n",
            "Requirement already satisfied: aiohttp in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.5)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.29)\n",
            "Requirement already satisfied: dataclasses-json in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.25.0)\n",
            "Requirement already satisfied: pandas in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (10.3.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (4.41.0.dev0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: scipy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: pydantic>=1.10 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.7.1)\n",
            "Requirement already satisfied: anyio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (4.3.0)\n",
            "Requirement already satisfied: certifi in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.4.28)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.12)\n",
            "Requirement already satisfied: jinja2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Downloading llama_index_embeddings_huggingface-0.2.0-py3-none-any.whl (7.1 kB)\n",
            "Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minijinja, llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.2.0 minijinja-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-embeddings-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZYvr8E9nH_z",
        "outputId": "9f1bbdc3-13d3-41c9-de09-7ab1f31016e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "973.61s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-vector-stores-chroma\n",
            "  Downloading llama_index_vector_stores_chroma-0.1.7-py3-none-any.whl.metadata (653 bytes)\n",
            "Requirement already satisfied: chromadb<0.6.0,>=0.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-vector-stores-chroma) (0.5.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-vector-stores-chroma) (0.10.33)\n",
            "Requirement already satisfied: build>=1.0.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.7.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.110.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.11.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.17.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.24.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.63.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.12.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.10.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.3.1)\n",
            "Requirement already satisfied: httpx in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.25.0)\n",
            "Requirement already satisfied: pandas in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (10.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.1.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.37.2)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.29.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.2.1)\n",
            "Requirement already satisfied: anyio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (4.3.0)\n",
            "Requirement already satisfied: httpcore==1.* in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.14.0)\n",
            "Requirement already satisfied: click in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.4.28)\n",
            "Requirement already satisfied: coloredlogs in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.25.3)\n",
            "Requirement already satisfied: sympy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.12)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.9.0)\n",
            "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (7.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.63.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.45b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.45b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (66.1.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests>=2.28->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.22.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (13.7.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (12.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.21.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.17.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.6.0)\n",
            "Downloading llama_index_vector_stores_chroma-0.1.7-py3-none-any.whl (4.8 kB)\n",
            "Installing collected packages: llama-index-vector-stores-chroma\n",
            "Successfully installed llama-index-vector-stores-chroma-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "980.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipywidgets) (8.24.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: decorator in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "987.60s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index-embeddings-ollama\n",
            "  Downloading llama_index_embeddings_ollama-0.1.2-py3-none-any.whl.metadata (654 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-embeddings-ollama) (0.10.33)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2024.3.1)\n",
            "Requirement already satisfied: httpx in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.8.1)\n",
            "Requirement already satisfied: numpy in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.25.0)\n",
            "Requirement already satisfied: pandas in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (10.3.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.9.4)\n",
            "Requirement already satisfied: pydantic>=1.10 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.7.1)\n",
            "Requirement already satisfied: anyio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (4.3.0)\n",
            "Requirement already satisfied: certifi in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.14.0)\n",
            "Requirement already satisfied: click in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2024.4.28)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/guillaume/git/oncoAIflow/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-ollama) (1.16.0)\n",
            "Downloading llama_index_embeddings_ollama-0.1.2-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: llama-index-embeddings-ollama\n",
            "Successfully installed llama-index-embeddings-ollama-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-embeddings-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lkHavErBXChp"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, ServiceContext, StorageContext, VectorStoreIndex\n",
        "\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "from llama_index.embeddings.ollama import OllamaEmbedding\n",
        "\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "import chromadb\n",
        "\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "eoT6JJbMOxB-"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient()\n",
        "chroma_collection = chroma_client.get_or_create_collection('OncoAIFlow') #climate_report\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWfc4Erqhms0",
        "outputId": "670ea2ba-7d24-4d0b-d0d0-2813a94ab7d9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COxp34FqXMih",
        "outputId": "e5d54073-f793-452e-9e1c-b604d69507b9"
      },
      "outputs": [],
      "source": [
        "# documents = SimpleDirectoryReader(\"rag_data\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KYmJUF8TXc-y"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "# system_prompt = \"Tu es un assistant medical. Tu dois repondre uniquement par oui ou par non. si tu n'est pas sur de toi, alors reponds par oui.\"\n",
        "# # This will wrap the default prompts that are internal to llama-index\n",
        "# query_wrapper_prompt = \"<|USER|>{query_str}<|ASSISTANT|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "7963d11236a9416c9c86215ac3695ad8",
            "298c31d5725645fb86588201fc6907a9",
            "585476f5f2094407a9c8c94ceaac1547",
            "bcfdf562870342ffa22b69e35cf24fb6",
            "efe6caaffe9e43b0b742c40a23e8db5b",
            "51d6034b2e374b0fa7ee22cd936ecdb9",
            "b39820f13ac7414fbad40cfca86606ee",
            "b7b0b9fd96934de9b6797c6751527e14",
            "eda96a6fdf7f49b698bafcd45a613e06",
            "b98dcdddb51d492392489ad96c4d53d6",
            "90e015d9a82040719f65707cd0d16cd6",
            "7615745b0acf402f82db34d0c8f4acf8",
            "8451e452cd1546b69b97a07a1f9e5174",
            "8da121ec3f3749f283fca73151b91516",
            "562793d4e52a4dc298c320a5a8427f80",
            "9f9d576464894b01a105848ea1fa4692",
            "52457f1bc89443d29e13fe2c44533656",
            "95bfc987ce92452dac465d22456271a3",
            "ec910980abac4c008ed34055c6220554",
            "3041c25d79c1465fbf3edf5a395084d5",
            "51247fc10282485e9cab932b9f5969fc",
            "fa0f10b22ae14c7c815dd997b3641f02",
            "a5d66e5b87e44dafb026b2c2f7a1dde3",
            "6c396ba8fb2c4c71bb7dbbe588a411fb",
            "e75697f20f8741e6ac45450fea8dff14",
            "a971da5d5216415190f187e92bf1ebf6",
            "27c57170757f4cd3b8653f89813aaf57",
            "40b9bf76ee7f4abba6784b4d6787f992",
            "4a1be73596f446b28c0a329140d47a56",
            "008f201663774b95b2664483de0cdc6f",
            "287d5162e13445498adb8c11188198ba",
            "bd4c251e47a74dfab672a2639175751e"
          ]
        },
        "id": "dtTmhDXUbxOj",
        "outputId": "4622f3f5-91e1-4b61-83d9-4f470da74b82"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acsts\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1BGUftGYp71",
        "outputId": "8dbe24ef-02f0-4402-b964-3adec2a6ae3c"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4zNsEmed5mS",
        "outputId": "31b4789b-49c1-408c-ce86-99a72fcac5d0"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "muyrP235a3eR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_default_device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "client = ollama.Client(host='http://localhost:11434')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'models': [{'name': 'Modelfile:latest',\n",
              "   'model': 'Modelfile:latest',\n",
              "   'modified_at': '2024-04-27T14:05:34.95895242Z',\n",
              "   'size': 4109865326,\n",
              "   'digest': '30227fd3ec36c3923a911e9ec2bd5d9bdf10b90db7333128a3803f1c52b36afc',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'llama',\n",
              "    'families': ['llama'],\n",
              "    'parameter_size': '7B',\n",
              "    'quantization_level': 'Q4_0'}},\n",
              "  {'name': 'mistral:7b',\n",
              "   'model': 'mistral:7b',\n",
              "   'modified_at': '2024-04-27T10:09:32.711597495Z',\n",
              "   'size': 4109865159,\n",
              "   'digest': '61e88e884507ba5e06c49b40e6226884b2a16e872382c2b44a42f2d119d804a5',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'llama',\n",
              "    'families': ['llama'],\n",
              "    'parameter_size': '7B',\n",
              "    'quantization_level': 'Q4_0'}},\n",
              "  {'name': 'mxbai-embed-large:latest',\n",
              "   'model': 'mxbai-embed-large:latest',\n",
              "   'modified_at': '2024-04-27T18:53:34.630969764Z',\n",
              "   'size': 669615493,\n",
              "   'digest': '468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'bert',\n",
              "    'families': ['bert'],\n",
              "    'parameter_size': '334M',\n",
              "    'quantization_level': 'F16'}},\n",
              "  {'name': 'ollama:latest',\n",
              "   'model': 'ollama:latest',\n",
              "   'modified_at': '2024-04-27T14:06:57.014334745Z',\n",
              "   'size': 4109865326,\n",
              "   'digest': '30227fd3ec36c3923a911e9ec2bd5d9bdf10b90db7333128a3803f1c52b36afc',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'llama',\n",
              "    'families': ['llama'],\n",
              "    'parameter_size': '7B',\n",
              "    'quantization_level': 'Q4_0'}},\n",
              "  {'name': 'test_embed:latest',\n",
              "   'model': 'test_embed:latest',\n",
              "   'modified_at': '2024-04-27T18:53:34.630969764Z',\n",
              "   'size': 669615493,\n",
              "   'digest': '6c3905879735a7d52925c9fc703f276197b39879db9e9d47f1cabf86f17ae8ef',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'bert',\n",
              "    'families': ['bert'],\n",
              "    'parameter_size': '334M',\n",
              "    'quantization_level': 'F16'}},\n",
              "  {'name': 'test_llm:latest',\n",
              "   'model': 'test_llm:latest',\n",
              "   'modified_at': '2024-04-28T09:08:57.147949319Z',\n",
              "   'size': 4109865471,\n",
              "   'digest': '8e3cdfbd956873bf049732e0cf11b79eaad4214b6d410df3b3a31502e64de296',\n",
              "   'details': {'parent_model': '',\n",
              "    'format': 'gguf',\n",
              "    'family': 'llama',\n",
              "    'families': ['llama'],\n",
              "    'parameter_size': '7B',\n",
              "    'quantization_level': 'Q4_0'}}]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "BhyzesSOYuRf",
        "outputId": "6f5a5a44-74d2-41d5-ae39-57ccbba4e49e"
      },
      "outputs": [],
      "source": [
        "# llm = Ollama(\n",
        "#     context_window=8000,\n",
        "#     max_new_tokens=256,\n",
        "#     generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
        "#     system_prompt=system_prompt,\n",
        "#     query_wrapper_prompt=query_wrapper_prompt,\n",
        "#     tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "#     model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "#     device_map=\"auto\",\n",
        "#     tokenizer_kwargs={\"max_length\": 8000},\n",
        "#     model_kwargs={\"torch_dtype\": torch.float16}\n",
        "\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'license': '                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.',\n",
              " 'modelfile': '# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM test_llm:latest\\n\\nFROM mistral:7b\\nTEMPLATE \"\"\"[INST] {{ .System }} {{ .Prompt }} [/INST]\"\"\"\\nSYSTEM \"\"\"Tu es un assistant medical. En te basant sur le chapitre 7 qui concerne les traitements, tu dois repondre uniquement par oui ou par non. si tu n\\'est pas sur de toi, alors reponds par oui.\"\"\"\\nPARAMETER num_ctx 8000\\nPARAMETER num_predict 256\\nPARAMETER stop \"[INST]\"\\nPARAMETER stop \"[/INST]\"\\nPARAMETER temperature 0.1',\n",
              " 'parameters': 'num_ctx                        8000\\nnum_predict                    256\\nstop                           \"[INST]\"\\nstop                           \"[/INST]\"\\ntemperature                    0.1',\n",
              " 'template': '[INST] {{ .System }} {{ .Prompt }} [/INST]',\n",
              " 'system': \"Tu es un assistant medical. En te basant sur le chapitre 7 qui concerne les traitements, tu dois repondre uniquement par oui ou par non. si tu n'est pas sur de toi, alors reponds par oui.\",\n",
              " 'details': {'parent_model': 'mistral:7b',\n",
              "  'format': 'gguf',\n",
              "  'family': 'llama',\n",
              "  'families': ['llama'],\n",
              "  'parameter_size': '7B',\n",
              "  'quantization_level': 'Q4_0'}}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ollama.show('test_llm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = Ollama(model=\"test_llm\", request_timeout = 1000)\n",
        "embed_model = OllamaEmbedding(model_name=\"test_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ollama.chat(model='Modelfile', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model': 'test_llm',\n",
              " 'created_at': '2024-04-28T09:09:22.837880783Z',\n",
              " 'response': \" Je suis désolé, je ne peux pas répondre à cette question car je ne suis pas un assistant médical et la question n'est pas liée au chapitre 7 sur les traitements que j'ai lu. Cependant, pour répondre à la question : Oui, le ciel apparaît bleu en raison de l'absorption selective de la lumière par l'atmosphère terrestre.\",\n",
              " 'done': True,\n",
              " 'context': [733,\n",
              "  16289,\n",
              "  28793,\n",
              "  16063,\n",
              "  1037,\n",
              "  521,\n",
              "  13892,\n",
              "  5714,\n",
              "  28723,\n",
              "  1618,\n",
              "  711,\n",
              "  2388,\n",
              "  440,\n",
              "  1147,\n",
              "  462,\n",
              "  11708,\n",
              "  279,\n",
              "  267,\n",
              "  28705,\n",
              "  28787,\n",
              "  3025,\n",
              "  14055,\n",
              "  485,\n",
              "  1514,\n",
              "  10610,\n",
              "  4485,\n",
              "  28725,\n",
              "  8582,\n",
              "  511,\n",
              "  278,\n",
              "  312,\n",
              "  19571,\n",
              "  267,\n",
              "  20988,\n",
              "  1116,\n",
              "  940,\n",
              "  3466,\n",
              "  28710,\n",
              "  3466,\n",
              "  940,\n",
              "  1843,\n",
              "  28723,\n",
              "  2144,\n",
              "  8582,\n",
              "  307,\n",
              "  28742,\n",
              "  374,\n",
              "  3557,\n",
              "  1147,\n",
              "  340,\n",
              "  298,\n",
              "  28710,\n",
              "  28725,\n",
              "  13086,\n",
              "  312,\n",
              "  766,\n",
              "  3673,\n",
              "  940,\n",
              "  3466,\n",
              "  28710,\n",
              "  28723,\n",
              "  4315,\n",
              "  349,\n",
              "  272,\n",
              "  7212,\n",
              "  5045,\n",
              "  28804,\n",
              "  733,\n",
              "  28748,\n",
              "  16289,\n",
              "  28793,\n",
              "  3291,\n",
              "  519,\n",
              "  278,\n",
              "  16018,\n",
              "  328,\n",
              "  28797,\n",
              "  28725,\n",
              "  2218,\n",
              "  435,\n",
              "  757,\n",
              "  1554,\n",
              "  3557,\n",
              "  3264,\n",
              "  19571,\n",
              "  267,\n",
              "  1289,\n",
              "  8821,\n",
              "  2996,\n",
              "  1253,\n",
              "  2218,\n",
              "  435,\n",
              "  519,\n",
              "  278,\n",
              "  3557,\n",
              "  521,\n",
              "  13892,\n",
              "  15454,\n",
              "  745,\n",
              "  911,\n",
              "  543,\n",
              "  2996,\n",
              "  307,\n",
              "  28742,\n",
              "  374,\n",
              "  3557,\n",
              "  635,\n",
              "  2110,\n",
              "  2505,\n",
              "  11708,\n",
              "  279,\n",
              "  267,\n",
              "  28705,\n",
              "  28787,\n",
              "  1147,\n",
              "  1514,\n",
              "  10610,\n",
              "  4485,\n",
              "  955,\n",
              "  461,\n",
              "  28742,\n",
              "  1585,\n",
              "  16899,\n",
              "  28723,\n",
              "  334,\n",
              "  26574,\n",
              "  28725,\n",
              "  2669,\n",
              "  3264,\n",
              "  19571,\n",
              "  267,\n",
              "  1289,\n",
              "  543,\n",
              "  2996,\n",
              "  714,\n",
              "  451,\n",
              "  1724,\n",
              "  28725,\n",
              "  462,\n",
              "  277,\n",
              "  755,\n",
              "  954,\n",
              "  2923,\n",
              "  21155,\n",
              "  8012,\n",
              "  28718,\n",
              "  481,\n",
              "  26765,\n",
              "  340,\n",
              "  305,\n",
              "  28742,\n",
              "  4737,\n",
              "  271,\n",
              "  768,\n",
              "  5339,\n",
              "  495,\n",
              "  340,\n",
              "  543,\n",
              "  17709,\n",
              "  8110,\n",
              "  940,\n",
              "  305,\n",
              "  28742,\n",
              "  270,\n",
              "  7186,\n",
              "  721,\n",
              "  3228,\n",
              "  3636,\n",
              "  3290,\n",
              "  267,\n",
              "  28723],\n",
              " 'total_duration': 5131283708,\n",
              " 'load_duration': 2879529375,\n",
              " 'prompt_eval_count': 72,\n",
              " 'prompt_eval_duration': 108921000,\n",
              " 'eval_count': 101,\n",
              " 'eval_duration': 2013170000}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ollama.generate(model='test_llm', prompt='Why is the sky blue?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6mT0mMqtctGl"
      },
      "outputs": [],
      "source": [
        "# embed_model =  HuggingFaceEmbedding(model_name=\"thenlper/gte-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EqUPeKH6crpM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_22076/186755162.py:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ],
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bwGjWnoNai9y"
      },
      "outputs": [],
      "source": [
        "# index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient()\n",
        "chroma_collection = chroma_client.get_collection('OncoAIFlow')\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "l2cSVsVGUsma"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7efbd964c6d0>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P738v53ganSZ"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b> Based on the context provided, a patient with a performance status of 4 and a unifocal CHC, along with Child-A liver disease, may not have an obvious choice between curative or palliative treatment. The decision might require the expertise of transplantation teams and radiology intervention specialists. Treatment options can include down-staging strategies for patients who do not meet transplantation criteria but could still benefit from individualized therapeutic approaches. However, the context does not mention systemic treatments as a potential option for this specific patient situation.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Est-ce que le traitement systemique fait partie des traitements possibles pour un patient un performance status a 4, porteur d'un CHC unifocal, avec une maladie hepatique CHILD A\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-m1a3zntIl3",
        "outputId": "18f998ba-9526-4529-8e7c-a77712d73d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la transplantation fait partie des traitements envisageable pour un patient un performance status a 1, porteur d'un CHC comportant trois nodules, avec cirrhose CHILD B.\n",
            "Est-ce que la destruction percutanee fait partie des traitements envisageable pour un patient un performance status a 3, porteur d'un CHC comportant trois nodules, sur foie non cirrhotique.\n",
            "Est-ce que la chimio-embolisation fait partie des traitements envisageable pour un patient un performance status a 0, porteur d'un CHC comportant de multiples nodules, sur foie non cirrhotique.\n",
            "Est-ce que la radiotherapie fait partie des traitements envisageable pour un patient un performance status a 2, porteur d'un CHC comportant de multiples nodules, avec cirrhose CHILD A.\n",
            "Est-ce que le traitement systemique fait partie des traitements envisageable pour un patient un performance status a 2, porteur d'un CHC comportant un nodule, avec cirrhose CHILD C.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Liste de valeurs pour les balises\n",
        "performance = ['0', '1', '2', '3', '4']\n",
        "traitement = ['la transplantation', 'la resection', 'la destruction percutanee', 'la radiotherapie', 'le traitement systemique', 'la chimio-embolisation', 'la radiotherapie interne selective']\n",
        "nodule = ['un nodule', 'deux nodules', 'trois nodules', 'de multiples nodules']\n",
        "maladie_hepatique = ['sur foie non cirrhotique','avec cirrhose CHILD A', 'avec cirrhose CHILD B', 'avec cirrhose CHILD C']\n",
        "\n",
        "# Fonction pour générer un exemple aléatoire\n",
        "def generer_prompt():\n",
        "    performance_aleatoire = random.choice(performance)\n",
        "    traitement_aleatoire = random.choice(traitement)\n",
        "    nodule_aleatoire = random.choice(nodule)\n",
        "    maladie_hepatique_aleatoire = random.choice(maladie_hepatique)\n",
        "    return f\"Est-ce que {traitement_aleatoire} fait partie des traitements envisageable pour un patient un performance status a {performance_aleatoire}, porteur d'un CHC comportant {nodule_aleatoire}, {maladie_hepatique_aleatoire}.\"\n",
        "\n",
        "# Exemple d'utilisation\n",
        "for _ in range(5):\n",
        "    print(generer_prompt())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8fYot8gCs2i6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b> Yes, based on the context provided, surgical resection is mentioned as one of the treatment options for a patient with a performance status of 1 and a unifocal CHC, assuming the liver disease is classified as CHILD A. However, it's important to note that specific eligibility for this treatment may depend on additional factors such as the tumor size, invasion vasculaire, and metastasis.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = query_engine.query(\"Est-ce que la resection chirurgicale fait partie des traitements possibles pour un patient un performance status a 1, porteur d'un CHC unifocal, avec une maladie hepatique CHILD A\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "TmMDlai_2tNt",
        "outputId": "2c65ac17-ab9c-4ffb-f04f-573bedb284f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la radiotherapie fait partie des traitements envisageable pour un patient un performance status a 0, porteur d'un CHC comportant un nodule, avec cirrhose CHILD A.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b> Yes, radiotherapy is an option for treating a patient with a performance status of 0 and a Child-A cirrhosis who has a hepatocellular carcinoma (CHC) with a nodule. The context information mentions that radiotherapies such as stereotaxic radiation therapy have been evaluated since the beginning of the 1990s and have shown favorable results in terms of local control and survival rates, even for larger CHCs. However, it's important to note that more data is necessary to broadly promote the use of non-thermal techniques like electroporation for non-ablative tumors.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la radiotherapie interne selective fait partie des traitements envisageable pour un patient un performance status a 2, porteur d'un CHC comportant un nodule, avec cirrhose CHILD B.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b> Based on the information provided in the context, a patient with a performance status of 2 and a Child-B cirrhosis, who is carrying a hepatocellular carcinoma (HCC) with a nodule, could be considered for various treatment options, including the radiotherapie interne selective (RIS). RIS involves the intra-arterial injection of microsphères, which are radioactive microspheres that provide both embolization and internal radiation therapy. There are different products based on Yttrium-90 (Y90) or Holium-166 (Ho166). The absence of a significant embolic effect makes it more appropriate to use the term RIS instead of radioembolization, although the extension of the portal vasculature is not a contraindication for this treatment. RIS has been tested in different stages of HCC and, in the early stages (BCLC A), it appears to have shown promising results. The concentration of radiation on one or two segments allows for high tumoral doses without significant concern for damage to healthy liver tissue. Clinical studies have reported high local response rates with minimal toxicity. However, longer</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la chimio-embolisation fait partie des traitements envisageable pour un patient un performance status a 4, porteur d'un CHC comportant de multiples nodules, avec cirrhose CHILD A.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b> Based on the information provided in the context, a patient with a performance status of 4 and a Child-A cirrhosis, who is carrying a multinodular hepatocellular carcinoma (CHC), may be considered for palliative treatments including chemicoembolization (CE). However, it's important to note that the response and efficacy can vary depending on the characteristics of the tumor, such as hypovasculature, size, multinodularity, elevated AFP or CRP levels. The choice between CE and systemic treatment can be guided by scores developed to help identify good candidates for CE. However, there is no universally validated and consensual score. The exact modalities of the CE (type of chemotherapy, embolization agent, use or absence of lipiodol, repetition frequency) and surveillance after treatment are not yet consensus. In general, selective or hyper-selective CE is favored to preserve more non-tumoral parenchyma, in accordance with ESMO recommendations.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la radiotherapie interne selective fait partie des traitements envisageable pour un patient un performance status a 1, porteur d'un CHC comportant deux nodules, avec cirrhose CHILD B.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b> Based on the information provided in the context, the radiotherapies mentioned include external beam radiation therapy (stereotaxic and intensive), radioembolization or selective internal radiation therapy (SIRT), and ablation techniques such as microwave ablation. The context mentions that SIRT has been tested in different stages of the disease, including in situation precoce (BCLC A) where it shows promising results. However, the specifics of the patient's case, such as performance status and presence of cirrhosis, were not mentioned in the context to definitively answer whether SIRT is a viable option for this particular patient. It would be best to consult with a healthcare professional for a proper assessment and treatment recommendation.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Est-ce que la radiotherapie interne selective fait partie des traitements envisageable pour un patient un performance status a 2, porteur d'un CHC comportant un nodule, avec cirrhose CHILD C.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "<b> Based on the information provided in the context, a treatment option for a patient with a performance status of 2 and a hepatocellular carcinoma (CHC) containing a nodule, with Child-Pugh Class C cirrhosis, is radiotherapy internal selective (RIS). This therapy involves the injection of radioisotope-carrying microspheres into the hepatic artery, followed by embolization and localized radiation therapy. It has been tested in various stages of the disease, including early-stage CHC, and has shown promising results in terms of tumor response and local control. However, it is essential to note that each case is unique, and individual patient factors should be carefully considered before making a treatment decision. Consultation with healthcare professionals experienced in liver cancer treatment is advised for personalized recommendations.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    query_engine = index.as_query_engine()\n",
        "    prompt = generer_prompt()\n",
        "    print(prompt)\n",
        "    response = query_engine.query(prompt)\n",
        "\n",
        "    display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008f201663774b95b2664483de0cdc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27c57170757f4cd3b8653f89813aaf57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287d5162e13445498adb8c11188198ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298c31d5725645fb86588201fc6907a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b0b9fd96934de9b6797c6751527e14",
            "placeholder": "​",
            "style": "IPY_MODEL_eda96a6fdf7f49b698bafcd45a613e06",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3041c25d79c1465fbf3edf5a395084d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b9bf76ee7f4abba6784b4d6787f992": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1be73596f446b28c0a329140d47a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51247fc10282485e9cab932b9f5969fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e75697f20f8741e6ac45450fea8dff14",
            "placeholder": "​",
            "style": "IPY_MODEL_a971da5d5216415190f187e92bf1ebf6",
            "value": "Token is valid (permission: write)."
          }
        },
        "51d6034b2e374b0fa7ee22cd936ecdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f9d576464894b01a105848ea1fa4692",
            "placeholder": "​",
            "style": "IPY_MODEL_52457f1bc89443d29e13fe2c44533656",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "52457f1bc89443d29e13fe2c44533656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562793d4e52a4dc298c320a5a8427f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "585476f5f2094407a9c8c94ceaac1547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b98dcdddb51d492392489ad96c4d53d6",
            "placeholder": "​",
            "style": "IPY_MODEL_90e015d9a82040719f65707cd0d16cd6",
            "value": ""
          }
        },
        "6c396ba8fb2c4c71bb7dbbe588a411fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_287d5162e13445498adb8c11188198ba",
            "placeholder": "​",
            "style": "IPY_MODEL_bd4c251e47a74dfab672a2639175751e",
            "value": "Login successful"
          }
        },
        "7615745b0acf402f82db34d0c8f4acf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7963d11236a9416c9c86215ac3695ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51247fc10282485e9cab932b9f5969fc",
              "IPY_MODEL_fa0f10b22ae14c7c815dd997b3641f02",
              "IPY_MODEL_a5d66e5b87e44dafb026b2c2f7a1dde3",
              "IPY_MODEL_6c396ba8fb2c4c71bb7dbbe588a411fb"
            ],
            "layout": "IPY_MODEL_b39820f13ac7414fbad40cfca86606ee"
          }
        },
        "8451e452cd1546b69b97a07a1f9e5174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8da121ec3f3749f283fca73151b91516": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e015d9a82040719f65707cd0d16cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95bfc987ce92452dac465d22456271a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec910980abac4c008ed34055c6220554",
            "placeholder": "​",
            "style": "IPY_MODEL_3041c25d79c1465fbf3edf5a395084d5",
            "value": "Connecting..."
          }
        },
        "9f9d576464894b01a105848ea1fa4692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5d66e5b87e44dafb026b2c2f7a1dde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a1be73596f446b28c0a329140d47a56",
            "placeholder": "​",
            "style": "IPY_MODEL_008f201663774b95b2664483de0cdc6f",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "a971da5d5216415190f187e92bf1ebf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39820f13ac7414fbad40cfca86606ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b7b0b9fd96934de9b6797c6751527e14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98dcdddb51d492392489ad96c4d53d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfdf562870342ffa22b69e35cf24fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7615745b0acf402f82db34d0c8f4acf8",
            "style": "IPY_MODEL_8451e452cd1546b69b97a07a1f9e5174",
            "value": true
          }
        },
        "bd4c251e47a74dfab672a2639175751e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e75697f20f8741e6ac45450fea8dff14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec910980abac4c008ed34055c6220554": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eda96a6fdf7f49b698bafcd45a613e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efe6caaffe9e43b0b742c40a23e8db5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8da121ec3f3749f283fca73151b91516",
            "style": "IPY_MODEL_562793d4e52a4dc298c320a5a8427f80",
            "tooltip": ""
          }
        },
        "fa0f10b22ae14c7c815dd997b3641f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c57170757f4cd3b8653f89813aaf57",
            "placeholder": "​",
            "style": "IPY_MODEL_40b9bf76ee7f4abba6784b4d6787f992",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
